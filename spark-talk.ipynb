{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.7.12 |Anaconda 4.2.0 (64-bit)| (default, Jul  2 2016, 17:42:40) \n",
      "[GCC 4.4.7 20120313 (Red Hat 4.4.7-1)]\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f465ea1ec10>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download and Subset Data\n",
    "\n",
    "We will first subset down the dataset of Amazon Book reviews located at [this link](http://snap.stanford.edu/data/amazon/productGraph/categoryFiles/reviews_Books_5.json.gz).  This dataset contains 8,898,041 book reviews.\n",
    "\n",
    "This dataset includes reviews (ratings, text, helpfulness votes), product metadata (descriptions, category information, price, brand, and image features), and links (also viewed/also bought graphs).  For more information please refer to [this page](http://jmcauley.ucsd.edu/data/amazon/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "url = \"s3n://galvanize-ds/reviews_Books_5.json.gz\"\n",
    "\n",
    "full_review_df = spark.read.json(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- asin: string (nullable = true)\n",
      " |-- helpful: array (nullable = true)\n",
      " |    |-- element: long (containsNull = true)\n",
      " |-- overall: double (nullable = true)\n",
      " |-- reviewText: string (nullable = true)\n",
      " |-- reviewTime: string (nullable = true)\n",
      " |-- reviewerID: string (nullable = true)\n",
      " |-- reviewerName: string (nullable = true)\n",
      " |-- summary: string (nullable = true)\n",
      " |-- unixReviewTime: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "full_review_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's subset our dataframe using the [sample DataFrame method](http://spark.apache.org/docs/2.1.0/api/python/pyspark.sql.html#pyspark.sql.DataFrame.sample) to only include 0.2% of the review text which will leave us with approximately 17,700 reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17513\n"
     ]
    }
   ],
   "source": [
    "review_subset = full_review_df.select('reviewText', 'overall') \\\n",
    "                              .sample(False, 0.002, 42)\n",
    "\n",
    "# View how many reviews are left\n",
    "print(review_subset.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------+\n",
      "|          reviewText|overall|\n",
      "+--------------------+-------+\n",
      "|Set in the Great ...|    4.0|\n",
      "|I don't know how ...|    5.0|\n",
      "|\"Water for Elepha...|    5.0|\n",
      "|I wanted to see h...|    5.0|\n",
      "|I found this very...|    4.0|\n",
      "|The ur-text for t...|    5.0|\n",
      "|I was told would ...|    5.0|\n",
      "|Too many threads ...|    3.0|\n",
      "|Totally great wri...|    5.0|\n",
      "|I won't rehash th...|    3.0|\n",
      "+--------------------+-------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "review_subset.show(10, truncate=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pyspark as ps    # for the pyspark suite\n",
    "from pyspark.sql.functions import udf, col\n",
    "from pyspark.sql.types import ArrayType, StringType\n",
    "import string\n",
    "import unicodedata\n",
    "\n",
    "from sklearn.feature_extraction.stop_words import ENGLISH_STOP_WORDS\n",
    "import spacy\n",
    "import numpy as np\n",
    "\n",
    "from pyspark.ml.feature import CountVectorizer\n",
    "from pyspark.ml.feature import IDF\n",
    "\n",
    "import sys\n",
    "\n",
    "\n",
    "def extract_bow_from_raw_text(text_as_string):\n",
    "    \"\"\" Extracts bag-of-words from a raw text string.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    text (str): a text document given as a string\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    list : the list of the tokens extracted and filtered from the text\n",
    "    \"\"\"\n",
    "    if (text_as_string == None):\n",
    "        return []\n",
    "\n",
    "    if (len(text_as_string) < 1):\n",
    "        return []\n",
    "\n",
    "    # Load nlp object if it isn't accessible\n",
    "    if 'nlp' not in globals():\n",
    "        global nlp\n",
    "        try:\n",
    "            # When running locally\n",
    "            nlp = spacy.load('en')\n",
    "        except RuntimeError:\n",
    "            # When running on AWS EMR Cluster\n",
    "            nlp = spacy.load('en', via='/mnt/spacy_en_data/')\n",
    "\n",
    "    # Run through spacy English module\n",
    "    doc = nlp(text_as_string)\n",
    "\n",
    "    # Part's of speech to keep in the result\n",
    "    pos_lst = ['ADJ', 'ADV', 'NOUN', 'PROPN', 'VERB']\n",
    "\n",
    "    # Lemmatize text and split into tokens\n",
    "    tokens = [token.lemma_.lower() for token in doc if token.pos_ in pos_lst]\n",
    "    \n",
    "    stop_words = {'book', 'author', 'read', \"'\", 'character'}.union(ENGLISH_STOP_WORDS)\n",
    "\n",
    "    # Remove stop words\n",
    "    no_stop_tokens = [token for token in tokens if token not in stop_words]\n",
    "\n",
    "    return(no_stop_tokens)\n",
    "\n",
    "\n",
    "def indexing_pipeline(input_df, **kwargs):\n",
    "    \"\"\" Runs a full text indexing pipeline on a collection of texts contained\n",
    "    in a DataFrame.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    input_df (DataFrame): a DataFrame that contains a field called 'text'\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    df : the same DataFrame with a column called 'features' for each document\n",
    "    wordlist : the list of words in the vocabulary with their corresponding IDF\n",
    "    \"\"\"\n",
    "    inputCol_ = kwargs.get(\"inputCol\", \"text\")\n",
    "    vocabSize_ = kwargs.get(\"vocabSize\", 5000)\n",
    "    minDF_ = kwargs.get(\"minDF\", 2.0)\n",
    "\n",
    "    tokenizer_udf = udf(extract_bow_from_raw_text, ArrayType(StringType()))\n",
    "    df_tokens = input_df.withColumn(\"bow\", tokenizer_udf(col(inputCol_)))\n",
    "\n",
    "    cv = CountVectorizer(inputCol=\"bow\", outputCol=\"vector_tf\", vocabSize=vocabSize_, minDF=minDF_)\n",
    "    cv_model = cv.fit(df_tokens)\n",
    "    df_features_tf = cv_model.transform(df_tokens)\n",
    "\n",
    "    idf = IDF(inputCol=\"vector_tf\", outputCol=\"features\")\n",
    "    idfModel = idf.fit(df_features_tf)\n",
    "    df_features = idfModel.transform(df_features_tf)\n",
    "\n",
    "    return(df_features, np.array(cv_model.vocabulary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- reviewText: string (nullable = true)\n",
      " |-- overall: double (nullable = true)\n",
      " |-- bow: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- vector_tf: vector (nullable = true)\n",
      " |-- features: vector (nullable = true)\n",
      "\n",
      "[u'book' u'read' u'story' u'love' u\"'\" u'character' u'make' u'just' u'time'\n",
      " u'good' u'really' u'life' u'think' u'know' u'great' u'author' u'write'\n",
      " u'way' u'want' u'series']\n"
     ]
    }
   ],
   "source": [
    "review_df, vocab = indexing_pipeline(review_subset, inputCol='reviewText')\n",
    "\n",
    "review_df.printSchema()\n",
    "review_df.persist()\n",
    "\n",
    "print(vocab[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train LDA Model\n",
    "\n",
    "Now that we have a DataFrame with column `features` containing a vector object representing the [Tf-Idf](https://en.wikipedia.org/wiki/Tf%E2%80%93idf) values for our words, we can apply the [Latent Dirichlet allocation algorithm contained in the `ml` package](http://spark.apache.org/docs/latest/api/python/pyspark.ml.html#pyspark.ml.clustering.LDA).\n",
    "\n",
    "For the sake of this demonstration we will be specifying 3 clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.clustering import LDA\n",
    "\n",
    "lda = LDA(k=10, seed=42, optimizer='em', featuresCol='features')\n",
    "model = lda.fit(review_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.defaultParallelism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.ml.clustering.DistributedLDAModel"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "model_description = model.describeTopics(20).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vocab = np.array(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top words associated with topic 0:\n",
      "[u\"'\" u'story' u'book' u'make' u'read' u'character' u'love' u'just' u'time'\n",
      " u'know' u'world' u'think' u'life' u'really' u'author' u'good' u'great'\n",
      " u'people' u'write' u'want']\n",
      "\n",
      "Top words associated with topic 1:\n",
      "[u'love' u'story' u'character' u\"'\" u'read' u'just' u'life' u'time'\n",
      " u'novel' u'feel' u'really' u'make' u'want' u'know' u'series' u'book'\n",
      " u'woman' u'think' u'man' u'enjoy']\n",
      "\n",
      "Top words associated with topic 2:\n",
      "[u'book' u\"'\" u'work' u'use' u'author' u'read' u'make' u'story' u'series'\n",
      " u'time' u'good' u'chapter' u'way' u'write' u'great' u'just' u'people'\n",
      " u'life' u'character' u'think']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for idx, row in model_description.iterrows():\n",
    "    print(\"Top words associated with topic {}:\".format(row['topic']))\n",
    "    print(\"{}\\n\".format(vocab[row['termIndices']]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
